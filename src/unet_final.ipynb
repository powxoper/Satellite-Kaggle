{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-07T08:32:46.253683",
     "start_time": "2017-03-07T08:32:44.165493"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import shapely.wkt\n",
    "import shapely.affinity\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize);\n",
    "\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "import random\n",
    "import datetime\n",
    "random.seed(0)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-07T08:32:46.260312",
     "start_time": "2017-03-07T08:32:46.254673"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "N_Cls = 10\n",
    "output = pd.read_csv(\"../input/sample_submission.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})\n",
    "image_size = 1024\n",
    "patch_size = 64\n",
    "smooth = 1e-12\n",
    "bands = ['M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Types\n",
    "\n",
    "1. Building\n",
    "2. Misc. Manmade Structure\n",
    "3. Road\n",
    "4. Track\n",
    "5. Trees\n",
    "6. Crops\n",
    "7. Waterways\n",
    "8. Standing water\n",
    "9. Vehicle Large\n",
    "10. Vehicle Small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:46.143450",
     "start_time": "2017-02-23T13:59:46.014760"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SatelliteImg(object):\n",
    "\n",
    "    def __init__(self, image_id, high, low, train = True):\n",
    "        img = self.get_img(image_id)\n",
    "        self.channel = img.shape[2]\n",
    "        self.img = np.zeros((image_size,image_size,self.channel))\n",
    "        # resize image to standard size = 1024*1024\n",
    "        for i in range(self.channel):   \n",
    "            image = img[:,:,i].clip(min=low[i], max=high[i])     \n",
    "            image = 255.0*(image - low[i])/ (high[i]-low[i]) \n",
    "            #image = img[:,:,i]\n",
    "            #\n",
    "            #self.img[:,:,i] = sp.misc.imresize(image, (image_size,image_size)) \n",
    "            self.img[:,:,i] = cv2.resize(image, (image_size,image_size)) \n",
    "        self.x_pixel, self.y_pixel = self.img.shape[:2]\n",
    "        self.img_size = self.x_pixel*self.y_pixel\n",
    "        for _im_id, _x, _y in csv.reader(open('../input/grid_sizes.csv')):\n",
    "            if _im_id == image_id:\n",
    "                self.x_max, self.y_min = float(_x), float(_y)\n",
    "                break   \n",
    "        self.x_scaler, self.y_scaler = self.get_scalers() \n",
    "        \n",
    "        if train == True:\n",
    "            self.train_polygons = [None]*10\n",
    "            self.train_mask = np.zeros((self.x_pixel, self.y_pixel, N_Cls))\n",
    "            #self.train_mask = np.zeros((image_size, image_size, N_Cls))\n",
    "            \n",
    "            for poly_type in range(0,N_Cls):\n",
    "                for _im_id, _poly_type, _poly in csv.reader(open('../input/train_wkt_v4.csv')):\n",
    "                    if _im_id == image_id and _poly_type == str(poly_type+1):                    \n",
    "                        self.train_polygons[poly_type] = shapely.wkt.loads(_poly)\n",
    "                        train_polygons_scaled = shapely.affinity.scale(\n",
    "                            self.train_polygons[poly_type], xfact=self.x_scaler, yfact=self.y_scaler, origin=(0, 0, 0))      \n",
    "                        self.train_mask[:,:,poly_type] = self.mask_for_polygons(train_polygons_scaled)\n",
    "                        break\n",
    "\n",
    "    def stretch_8bit(self, bands, lower_percent=2, higher_percent=98):\n",
    "        out = np.zeros_like(bands)\n",
    "        x,y,z = bands.shape\n",
    "        for i in range(z):\n",
    "            a = 0 \n",
    "            b = 255 \n",
    "            c = np.percentile(bands[:,:,i], lower_percent)\n",
    "            d = np.percentile(bands[:,:,i], higher_percent)        \n",
    "            t = a + (bands[:,:,i] - c) * (b - a) / (d - c)    \n",
    "            t[t<a] = a\n",
    "            t[t>b] = b\n",
    "            out[:,:,i] = t\n",
    "        return out.astype(np.uint8)    \n",
    "    \n",
    "    def get_img(self, image_id):\n",
    "        filename = os.path.join('..', 'input', 'sixteen_band', '{}_M.tif'.format(image_id))\n",
    "        img = tiff.imread(filename)    \n",
    "        img = np.rollaxis(img, 0, 3)\n",
    "        return img\n",
    "    \n",
    "    def get_scalers(self):\n",
    "        w = float(self.y_pixel)\n",
    "        h = float(self.x_pixel)\n",
    "        w_ = w * (w / (w + 1))\n",
    "        h_ = h * (h / (h + 1))\n",
    "        return w_ / self.x_max, h_ / self.y_min\n",
    "    \n",
    "    def mask_for_polygons(self, polygons):\n",
    "        img_mask = np.zeros((self.x_pixel,self.y_pixel), np.uint8)\n",
    "        if not polygons:\n",
    "            return img_mask\n",
    "        int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "        interiors = [int_coords(pi.coords) for poly in polygons for pi in poly.interiors]\n",
    "        cv2.fillPoly(img_mask, exteriors, 1)\n",
    "        cv2.fillPoly(img_mask, interiors, 0)\n",
    "        return img_mask\n",
    "\n",
    "    def show_img(self):\n",
    "        m = self.img\n",
    "        img = np.zeros_like(m[:,:,0:3])\n",
    "        img[:,:,0] = m[:,:,4] #red\n",
    "        img[:,:,1] = m[:,:,2] #green\n",
    "        img[:,:,2] = m[:,:,1] #blue\n",
    "        img = self.stretch_8bit(img) # simple strech is probably not a good idea for learning        \n",
    "        return plt.imshow(img) \n",
    "     \n",
    "    def show_mask(self, poly_type=1):\n",
    "        #train_mask = self.mask_for_polygons(self.train_polygons_scaled[poly_type])\n",
    "        train_mask = self.train_mask[:,:,poly_type-1]\n",
    "        return tiff.imshow(255 * np.stack([train_mask, train_mask, train_mask]));    \n",
    "    \n",
    "    def get_polygon(self, poly_type=1):\n",
    "        return self.train_polygons[poly_type-1]\n",
    "    \n",
    "    def get_data(self):\n",
    "        #img = self.stretch_8bit(self.img)\n",
    "        img = self.img\n",
    "        img = img.reshape((self.x_pixel*self.y_pixel, self.channel))\n",
    "        df = pd.DataFrame(img,index=range(self.img_size))\n",
    "        return df\n",
    "\n",
    "    def get_patch(self):\n",
    "        img = self.img\n",
    "        mask = self.train_mask   \n",
    "        x, y, label = [], [], []\n",
    "        #tr0 = [0.4, 0.1, 0.1, 0.15, 0.3, 0.95, 0.1, 0.05, 0.001, 0.005] # threshold = avg fraction of pixel in each class??\n",
    "        #tr = [item/2.0 for item in tr0] \n",
    "        tr = [0.04, 0.01, 0.009, 0.03, 0.12, 0.27, 0.005, 0.002, 0.00003, 0.00036] \n",
    "        no_patch = int(1.0*image_size/patch_size)\n",
    "        \n",
    "        for i in range(no_patch):\n",
    "            for j in range(no_patch):\n",
    "                xc = i*patch_size\n",
    "                yc = j*patch_size\n",
    "                im = img[xc:xc + patch_size, yc:yc + patch_size,:] \n",
    "                ms = mask[xc:xc + patch_size, yc:yc + patch_size,:]\n",
    "                obj = []\n",
    "                for k in range(N_Cls):\n",
    "                    sm = np.sum(ms[:, :, k])\n",
    "                    frac = float(sm) / (patch_size**2)\n",
    "                    obj.append(frac) \n",
    "                #if any(a>b for a,b in zip(obj,tr)):\n",
    "                if True:\n",
    "                    x.append(im)\n",
    "                    y.append(ms)\n",
    "                    test = [float(m - n)/n for m,n in zip(obj,tr)]\n",
    "                    label.append(test.index(max(test)))\n",
    "                  \n",
    "        return x, y, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:46.502500",
     "start_time": "2017-02-23T13:59:46.445273"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mask_to_polygons(mask, epsilon=5., min_area=1.):\n",
    "    # first, find contours with cv2: it's much faster than shapely\n",
    "    image, contours, hierarchy = cv2.findContours(((mask == 1) * 255).astype(np.uint8), cv2.RETR_CCOMP, \n",
    "                                                  cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "    # create approximate contours to have reasonable submission size\n",
    "    approx_contours = [cv2.approxPolyDP(cnt, epsilon, True)\n",
    "                       for cnt in contours]\n",
    "    if not contours:\n",
    "        return MultiPolygon()\n",
    "    # now messy stuff to associate parent and child contours\n",
    "    cnt_children = defaultdict(list)\n",
    "    child_contours = set()\n",
    "    assert hierarchy.shape[0] == 1\n",
    "    # http://docs.opencv.org/3.1.0/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "    for idx, (_, _, _, parent_idx) in enumerate(hierarchy[0]):\n",
    "        if parent_idx != -1:\n",
    "            child_contours.add(idx)\n",
    "            cnt_children[parent_idx].append(approx_contours[idx])\n",
    "    # create actual polygons filtering by area (removes artifacts)\n",
    "    all_polygons = []\n",
    "    for idx, cnt in enumerate(approx_contours):\n",
    "        if idx not in child_contours and cv2.contourArea(cnt) >= min_area:\n",
    "            assert cnt.shape[1] == 1\n",
    "            poly = Polygon(\n",
    "                shell=cnt[:, 0, :],\n",
    "                holes=[c[:, 0, :] for c in cnt_children.get(idx, [])\n",
    "                       if cv2.contourArea(c) >= min_area])\n",
    "            all_polygons.append(poly)\n",
    "    # approximating polygons might have created invalid ones, fix them\n",
    "    all_polygons = MultiPolygon(all_polygons)\n",
    "    if not all_polygons.is_valid:\n",
    "        all_polygons = all_polygons.buffer(0)\n",
    "        # Sometimes buffer() converts a simple Multipolygon to just a Polygon,\n",
    "        # need to keep it a Multi throughout\n",
    "        if all_polygons.type == 'Polygon':\n",
    "            all_polygons = MultiPolygon([all_polygons])\n",
    "    return all_polygons\n",
    "\n",
    "def to_polygons(mask, instance, epsilon=5., min_area=1.):\n",
    "    label_img = mask.reshape(instance.x_pixel, instance.y_pixel)\n",
    "    pred_polygons = mask_to_polygons(label_img, epsilon=epsilon, min_area=min_area)\n",
    "    scaled_pred_polygons = shapely.affinity.scale(\n",
    "    pred_polygons, xfact=1 / instance.x_scaler, yfact=1 / instance.y_scaler, origin=(0, 0, 0))\n",
    "    dumped_prediction = shapely.wkt.dumps(scaled_pred_polygons)\n",
    "    final_polygons = shapely.wkt.loads(dumped_prediction)\n",
    "    return final_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:46.944961",
     "start_time": "2017-02-23T13:59:46.939753"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# clipping range for 8 band image (percentile 99.9)\n",
    "low = [224.0, 190.0, 212.0, 181.0, 100.0, 189.0, 198.0, 138.0]\n",
    "high = [429.0, 655.0, 937.0, 834.0, 844.0, 836.0, 1162.0, 798.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:48.065880",
     "start_time": "2017-02-23T13:59:48.058825"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_list = ['6010_1_2', '6010_4_2', '6010_4_4', '6040_1_0', '6040_1_3', '6040_2_2', '6040_4_4', '6060_2_3', \n",
    "              '6070_2_3', '6090_2_0', '6100_1_3', '6100_2_2', '6100_2_3', '6110_1_2', '6110_3_1', '6110_4_0', \n",
    "              '6120_2_0', '6120_2_2', '6140_1_2', '6140_3_1', '6150_2_3', '6160_2_1', '6170_0_4', '6170_2_4', \n",
    "              '6170_4_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Getting clipping range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-11T18:22:10.638890",
     "start_time": "2017-02-11T18:22:10.630059"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SatelliteImg2(object):\n",
    "\n",
    "    def __init__(self, image_id):\n",
    "        self.img = self.get_img(image_id)       \n",
    "        self.x_pixel, self.y_pixel = self.img.shape[:2]\n",
    "        self.img_size = self.x_pixel*self.y_pixel\n",
    "        self.channel = self.img.shape[2]\n",
    "        \n",
    "    def get_img(self, image_id):\n",
    "        filename = os.path.join('..', 'input', 'sixteen_band', '{}_M.tif'.format(image_id))\n",
    "        img = tiff.imread(filename)    \n",
    "        img = np.rollaxis(img, 0, 3)\n",
    "        return img\n",
    "        \n",
    "    def get_data(self):\n",
    "        img = self.img\n",
    "        img = img.reshape((self.x_pixel*self.y_pixel, self.channel))\n",
    "        df = pd.DataFrame(img,index=range(self.img_size))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-09T10:19:31.914702",
     "start_time": "2017-02-09T10:19:29.274500"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_train = []\n",
    "df_train_part = []\n",
    "\n",
    "for i, trainable in enumerate(train_list): \n",
    "    image_train.append(SatelliteImg2(trainable))\n",
    "    df_train_part.append(image_train[i].get_data())\n",
    "    \n",
    "df_train = pd.concat(df_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-09T10:19:33.977429",
     "start_time": "2017-02-09T10:19:31.915588"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "low = df_train.quantile(q=0.001)\n",
    "high = df_train.quantile(q=0.999)\n",
    "\n",
    "for i,column in enumerate(list(df_train.columns.values)):\n",
    "    df_train[column] = df_train[column].clip(lower=low[i],upper=high[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-09T10:19:33.980539",
     "start_time": "2017-02-09T10:19:33.978284"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-09T10:19:41.364250",
     "start_time": "2017-02-09T10:19:41.354851"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "low = [224.0, 190.0, 212.0, 181.0, 100.0, 189.0, 198.0, 138.0]\n",
    "high = [429.0, 655.0, 937.0, 834.0, 844.0, 836.0, 1162.0, 798.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-10T11:01:10.452968",
     "start_time": "2017-02-10T11:01:08.978985"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp = SatelliteImg('6120_2_2', high = high, low = low )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T08:11:42.833075",
     "start_time": "2017-02-22T08:11:10.831415"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_train = []\n",
    "train_set = []\n",
    "target_set = []\n",
    "label = []\n",
    "\n",
    "for i, trainable in enumerate(train_list): \n",
    "    image_train.append(SatelliteImg(trainable, high = high, low = low))\n",
    "    x,y, cl = image_train[i].get_patch()   \n",
    "    train_set += x\n",
    "    target_set += y\n",
    "    label += cl\n",
    "    \n",
    "train_set = np.array(train_set).transpose(0,3,1,2)\n",
    "target_set = np.array(target_set).transpose(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T08:11:42.836980",
     "start_time": "2017-02-22T08:11:42.834262"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Make Validation (temporary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T08:11:42.890949",
     "start_time": "2017-02-22T08:11:42.837686"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label_mark = [None]*N_Cls\n",
    "for search in range(N_Cls):\n",
    "    label_mark[search] = np.where(np.array(label) == search)[0]\n",
    "\n",
    "for x in label_mark:\n",
    "    print len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T08:11:43.714755",
     "start_time": "2017-02-22T08:11:42.891975"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "target = []\n",
    "train_val = []\n",
    "target_val = []\n",
    "val_frac = 0.2\n",
    "\n",
    "for i in range(N_Cls):\n",
    "    my_list = list(label_mark[i])\n",
    "    val_size = int(val_frac*len(my_list))\n",
    "    random.shuffle(my_list)\n",
    "    for j in range(val_size):\n",
    "        item_no = my_list.pop()      \n",
    "        train_val.append(train_set[item_no])\n",
    "        target_val.append(target_set[item_no])\n",
    "    for k in my_list:\n",
    "        train.append(train_set[k])\n",
    "        target.append(target_set[k])\n",
    "  \n",
    "train = np.array(train)\n",
    "target = np.array(target)\n",
    "train_val = np.array(train_val)\n",
    "target_val = np.array(target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T08:11:43.722440",
     "start_time": "2017-02-22T08:11:43.717659"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T08:11:43.769105",
     "start_time": "2017-02-22T08:11:43.724233"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:52.232430",
     "start_time": "2017-02-23T13:59:52.205475"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_jaccard(img_a,img_b):\n",
    "    intersection = np.sum(img_a * img_b, axis=(0, -1, -2))\n",
    "    sum_ = np.sum(img_a + img_b, axis=(0, -1, -2))\n",
    "    jrk = (intersection + smooth) / (sum_ - intersection + smooth)       \n",
    "    return jrk\n",
    "\n",
    "def calc_jacc(model, train, target, finetune = 10):\n",
    "    img = train\n",
    "    msk = target\n",
    "\n",
    "    prd = model.predict(img, batch_size=4)\n",
    "    jk_list, threshold = [], []\n",
    "\n",
    "    for i in range(N_Cls):\n",
    "        t_msk = msk[:, i, :, :]\n",
    "        t_prd = prd[:, i, :, :]\n",
    "\n",
    "        m, b_tr = 0, 0\n",
    "        for j in range(finetune):\n",
    "            tr = float(j) / finetune\n",
    "            pred_binary_mask = t_prd > tr\n",
    "            jk = my_jaccard(t_msk, pred_binary_mask)\n",
    "            if jk > m:\n",
    "                m = jk\n",
    "                b_tr = tr\n",
    "        #print i, m, b_tr\n",
    "        jk_list.append(m)\n",
    "        threshold.append(b_tr)\n",
    "\n",
    "    score = sum(jk_list) / 10.0\n",
    "    #print 'jaccard: ',score\n",
    "    return score, threshold, jk_list\n",
    "\n",
    "def calc_jacc_thres(model, train, target, threshold= [0.4, 0.1, 0.4, 0.3, 0.3, 0.5, 0.3, 0.6, 0.1, 0.1]):\n",
    "    img = train\n",
    "    msk = target\n",
    "\n",
    "    prd = model.predict(img, batch_size=4)\n",
    "    jk_list = []\n",
    "\n",
    "    for i in range(N_Cls):\n",
    "        t_msk = msk[:, i, :, :]\n",
    "        t_prd = prd[:, i, :, :]\n",
    "\n",
    "        tr = threshold[i]\n",
    "        pred_binary_mask = t_prd > tr\n",
    "        jk = my_jaccard(t_msk, pred_binary_mask)       \n",
    "        jk_list.append(jk)\n",
    "\n",
    "    score = sum(jk_list) / 10.0\n",
    "    return score, jk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:53.705526",
     "start_time": "2017-02-23T13:59:53.659688"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def unet_down(inputs, dim, drop_rate = 0):\n",
    "    norm = BatchNormalization(axis=1)(inputs)\n",
    "    drop = Dropout(drop_rate)(norm)\n",
    "    conv = Convolution2D(dim, 3, 3, activation='relu', border_mode='same')(drop)\n",
    "    conv = Convolution2D(dim, 3, 3, activation='relu', border_mode='same')(conv)\n",
    "    pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    return conv, pool\n",
    "\n",
    "def unet_up(inputs, to_merge, dim, drop_rate = 0.2):\n",
    "    up = merge([UpSampling2D(size=(2, 2))(inputs), to_merge], mode='concat', concat_axis=1)\n",
    "    norm = BatchNormalization(axis=1)(up)\n",
    "    drop = Dropout(drop_rate)(norm)\n",
    "    conv = Convolution2D(dim, 3, 3, activation='relu', border_mode='same')(drop)\n",
    "    conv = Convolution2D(dim, 3, 3, activation='relu', border_mode='same')(conv)\n",
    "    return conv\n",
    "\n",
    "def get_unet():\n",
    "    inputs = Input((8, patch_size, patch_size))\n",
    " \n",
    "    conv1, layer1 = unet_down(inputs, 32)\n",
    "    conv2, layer2 = unet_down(layer1, 64)\n",
    "    conv3, layer3 = unet_down(layer2, 128)\n",
    "    conv4, layer4 = unet_down(layer3, 256)\n",
    "    \n",
    "    norm5 = BatchNormalization(axis=1)(layer4)\n",
    "    conv5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(norm5)\n",
    "    layer5 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv5)\n",
    "\n",
    "    layer6 = unet_up(layer5, conv4 , 256,drop_rate=0.1)\n",
    "    layer7 = unet_up(layer6, conv3, 128,drop_rate=0.2)    \n",
    "    layer8 = unet_up(layer7, conv2, 64,drop_rate=0.3)\n",
    "    layer9 = unet_up(layer8, conv1, 32,drop_rate=0.4)\n",
    "    \n",
    "    conv10 = Convolution2D(N_Cls, 1, 1, activation='sigmoid')(layer9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[jaccard_coef_int])\n",
    "    return model\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = y_pred>0.5\n",
    "    \n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)\n",
    "\n",
    "def train_model(model, n=1):\n",
    "    tr_list = np.array([0.0]*10)\n",
    "    val_tr_list = np.array([0.0]*10)\n",
    "\n",
    "    for i in range(n):\n",
    "        print 'epoch:', i+1\n",
    "        model.fit(train, target, batch_size=16, nb_epoch=1, verbose=2, shuffle=True,\n",
    "                  validation_data=(train_val,target_val))\n",
    "        train_score, train_threshold, train_jk_list = calc_jacc(model, train = train, target = target)\n",
    "        print 'train_jaccard: ', train_score\n",
    "        val_score, val_threshold, val_jk_list = calc_jacc(model, train = train_val, target = target_val)\n",
    "        print 'val_jaccard: ', val_score\n",
    "        tr_list += np.array(train_threshold)\n",
    "        val_tr_list += np.array(val_threshold)\n",
    "      \n",
    "    tr_list /= n\n",
    "    val_tr_list /=n\n",
    "    \n",
    "    return val_score, val_jk_list, tr_list, val_tr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:54.966723",
     "start_time": "2017-02-23T13:59:54.390730"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = get_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-10T22:19:14.451308",
     "start_time": "2017-02-10T22:19:14.445758"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T13:59:57.496642",
     "start_time": "2017-02-23T13:59:57.491279"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# good old threshold\n",
    "val_tr_list = np.array([ 0.386,  0.198,  0.45 ,  0.258,  0.408,  0.514,  0.46 ,  0.192,\n",
    "        0.008,  0.07 ])\n",
    "tr_list = np.array([ 0.378,  0.2  ,  0.608,  0.324,  0.414,  0.496,  0.482,  0.442,\n",
    "        0.098,  0.09 ])\n",
    "threshold = 0.2*tr_list + 0.8*val_tr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T09:32:11.612270",
     "start_time": "2017-02-22T08:15:37.257189"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(train, target, batch_size=16, nb_epoch=100, verbose=1, shuffle=True, validation_data=(train_val,target_val))\n",
    "model.save_weights('../weights/unet37_100e')\n",
    "calc_jacc_thres(model, train_val,target_val, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T09:52:47.761552",
     "start_time": "2017-02-22T09:32:11.612999"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(train, target, batch_size=16, nb_epoch=25, verbose=1, shuffle=True, validation_data=(train_val,target_val))\n",
    "model.save_weights('../weights/unet37_125e')\n",
    "calc_jacc_thres(model, train_val,target_val, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T20:38:36.858490",
     "start_time": "2017-02-22T20:38:36.825026"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('../weights/unet37_125e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T10:13:23.673760",
     "start_time": "2017-02-22T09:52:47.762357"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(train, target, batch_size=16, nb_epoch=25, verbose=1, shuffle=True, validation_data=(train_val,target_val))\n",
    "model.save_weights('../weights/unet37_150e')\n",
    "calc_jacc_thres(model, train_val,target_val, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T13:38:22.227355",
     "start_time": "2017-02-15T13:38:22.221441"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tr_list = np.array([ 0.378,  0.2  ,  0.608,  0.324,  0.414,  0.496,  0.482,  0.442,\n",
    "        0.098,  0.09 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T13:38:22.593529",
     "start_time": "2017-02-15T13:38:22.588085"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.8*tr_list+0.2*val_tr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-09T22:06:07.503265",
     "start_time": "2017-02-09T22:05:47.597854"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "score, threshold, jk_list = calc_jacc(model, train = train, target = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:00:02.418337",
     "start_time": "2017-02-23T14:00:02.325117"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "%cd ../input/sixteen_band/\n",
    "g = glob('*_M.tif')\n",
    "%cd $cwd\n",
    "all_list = [item[:8] for item in g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:00:02.827901",
     "start_time": "2017-02-23T14:00:02.820708"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"../input/sample_submission.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:00:03.434160",
     "start_time": "2017-02-23T14:00:03.429608"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_list = [item for item in all_list if item not in train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:00:03.719398",
     "start_time": "2017-02-23T14:00:03.707659"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, img, threshold = [0.4, 0.1, 0.4, 0.3, 0.3, 0.5, 0.3, 0.6, 0.1, 0.1]):\n",
    "    no_patch = int(1.0*image_size/patch_size)\n",
    "      \n",
    "    mask = np.zeros((N_Cls,image_size,image_size))    \n",
    "        \n",
    "    for i in range(no_patch):\n",
    "        for j in range(no_patch):\n",
    "            xc = i*patch_size\n",
    "            yc = j*patch_size\n",
    "            im = img[xc:xc + patch_size, yc:yc + patch_size,:] \n",
    "            im = np.transpose(im, (2,0,1))\n",
    "            im = np.expand_dims(im,axis=0)\n",
    "            patch_msk = model.predict(im)\n",
    "            mask[:,xc:xc + patch_size, yc:yc + patch_size] = patch_msk[0,:,:,:]\n",
    "            \n",
    "    mask = np.transpose(mask, (1,2,0)) \n",
    "    \n",
    "    for i in range(N_Cls): #predict above threshold\n",
    "        mask[:,:,i] = mask[:,:,i] > threshold[i]\n",
    "  \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T20:48:04.201207",
     "start_time": "2017-02-22T20:39:39.905148"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test in test_list:\n",
    "    image_test = SatelliteImg(test, high = high, low = low, train = False)\n",
    "    data = image_test.img\n",
    "    pred = predict(model, data, threshold)\n",
    "    print test\n",
    "    for i in range(N_Cls):        \n",
    "        result = to_polygons(pred[:,:,i], image_test, epsilon=1., min_area=5.)\n",
    "        output.set_value((output['ImageId']==test) & (output['ClassType']==i+1),'MultipolygonWKT', str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T20:48:05.579175",
     "start_time": "2017-02-22T20:48:04.202055"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.to_csv('../output/my_unet37_125e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T13:50:12.493649",
     "start_time": "2017-02-15T13:50:12.482149"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_jaccard(img_a,img_b):\n",
    "    intersection = np.sum(img_a * img_b, axis=(0, -1, -2))\n",
    "    sum_ = np.sum(img_a + img_b, axis=(0, -1, -2))\n",
    "    jrk = (intersection + smooth) / (sum_ - intersection + smooth)       \n",
    "    return jrk\n",
    "def calc_jacc_thres(model, train, target, threshold= [0.4, 0.1, 0.4, 0.3, 0.3, 0.5, 0.3, 0.6, 0.1, 0.1]):\n",
    "    img = train\n",
    "    msk = target\n",
    "\n",
    "    prd = model.predict(img, batch_size=4)\n",
    "    jk_list = []\n",
    "\n",
    "    for i in range(N_Cls):\n",
    "        t_msk = msk[:, i, :, :]\n",
    "        t_prd = prd[:, i, :, :]\n",
    "\n",
    "        tr = threshold[i]\n",
    "        pred_binary_mask = t_prd > tr\n",
    "        jk = my_jaccard(t_msk, pred_binary_mask)       \n",
    "        jk_list.append(jk)\n",
    "\n",
    "    score = sum(jk_list) / 10.0\n",
    "    #print 'jaccard: ',score\n",
    "    return score, jk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T13:50:39.837231",
     "start_time": "2017-02-15T13:50:37.830561"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_jacc_thres(model, train_val, target_val, threshold = threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## second part: building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:15:56.810648",
     "start_time": "2017-03-06T10:15:55.685637"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"../output/my_unet37_125e.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})\n",
    "output2 = pd.read_csv(\"../output/my_unet40_c1_test.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:16:46.925242",
     "start_time": "2017-03-06T10:16:46.913049"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:16:25.335874",
     "start_time": "2017-03-06T10:16:25.323764"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output[output[\"ClassType\"]==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:16:33.467632",
     "start_time": "2017-03-06T10:16:33.459340"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output2[output2[\"ClassType\"]==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:20:25.619411",
     "start_time": "2017-03-06T10:20:23.046262"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_list = list(output['ImageId'].unique())\n",
    "\n",
    "for i,test in enumerate(test_list):\n",
    "    new = output2[(output2['ImageId']==test) & (output2['ClassType']==1)][\"MultipolygonWKT\"]\n",
    "    output.set_value((output['ImageId']==test) & (output['ClassType']==1),'MultipolygonWKT', new)\n",
    "        \n",
    "output.to_csv('../output/my_unet37extra.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:20:51.063377",
     "start_time": "2017-03-06T10:20:51.047476"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output[output[\"ClassType\"]==1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Third part: building + waterway + stillwater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:56:14.945691",
     "start_time": "2017-03-06T10:56:12.000507"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"../output/my_unet37extra.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})\n",
    "output2 = pd.read_csv(\"subm/3.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:58:21.608878",
     "start_time": "2017-03-06T10:58:21.599167"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output[(output[\"ClassType\"] == 7) | (output[\"ClassType\"] == 8)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:58:11.275681",
     "start_time": "2017-03-06T10:58:11.268994"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output2[(output2[\"ClassType\"] == 7) | (output2[\"ClassType\"] == 8)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:59:36.190825",
     "start_time": "2017-03-06T10:59:32.419674"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_list = list(output['ImageId'].unique())\n",
    "\n",
    "for i,test in enumerate(test_list):\n",
    "    new = output2[(output2['ImageId']==test) & (output2['ClassType']==7)][\"MultipolygonWKT\"]\n",
    "    output.set_value((output['ImageId']==test) & (output['ClassType']==7),'MultipolygonWKT', new)\n",
    "    new = output2[(output2['ImageId']==test) & (output2['ClassType']==8)][\"MultipolygonWKT\"]\n",
    "    output.set_value((output['ImageId']==test) & (output['ClassType']==8),'MultipolygonWKT', new)\n",
    "        \n",
    "output.to_csv('../output/my_unet37extra2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-06T10:59:40.966659",
     "start_time": "2017-03-06T10:59:40.956490"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output[(output[\"ClassType\"] == 7) | (output[\"ClassType\"] == 8)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Forth part: building, man-made, road, track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-07T08:32:54.911411",
     "start_time": "2017-03-07T08:32:52.480138"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"../output/my_unet37extra.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})\n",
    "output2 = pd.read_csv(\"../output/final1.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-07T08:33:49.350271",
     "start_time": "2017-03-07T08:33:49.318978"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output[(output[\"ClassType\"] == 2) | (output[\"ClassType\"] == 3) | (output[\"ClassType\"] == 4) | (output[\"ClassType\"] == 5)].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-07T08:33:53.307349",
     "start_time": "2017-03-07T08:33:53.276264"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output2[(output2[\"ClassType\"] == 2) | (output2[\"ClassType\"] == 3) | (output2[\"ClassType\"] == 4) | (output2[\"ClassType\"] == 5)].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-07T08:35:34.207527",
     "start_time": "2017-03-07T08:35:29.082378"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_list = list(output['ImageId'].unique())\n",
    "\n",
    "for i,test in enumerate(test_list):\n",
    "    new = output2[(output2['ImageId']==test) & (output2['ClassType']==2)][\"MultipolygonWKT\"]\n",
    "    output.set_value((output['ImageId']==test) & (output['ClassType']==2),'MultipolygonWKT', new)\n",
    "    new = output2[(output2['ImageId']==test) & (output2['ClassType']==3)][\"MultipolygonWKT\"]\n",
    "    output.set_value((output['ImageId']==test) & (output['ClassType']==3),'MultipolygonWKT', new)\n",
    "    new = output2[(output2['ImageId']==test) & (output2['ClassType']==4)][\"MultipolygonWKT\"]\n",
    "    output.set_value((output['ImageId']==test) & (output['ClassType']==4),'MultipolygonWKT', new)\n",
    "        \n",
    "output.to_csv('../output/final2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-07T08:35:45.111679",
     "start_time": "2017-03-07T08:35:45.079928"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output[(output[\"ClassType\"] == 2) | (output[\"ClassType\"] == 3) | (output[\"ClassType\"] == 4) | (output[\"ClassType\"] == 5)].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Repair error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T21:16:33.675320",
     "start_time": "2017-02-22T21:16:33.645268"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat Dec 31 12:22:50 2016\n",
    "\n",
    "@author: ironbar\n",
    "\n",
    "Function for dealing with topology exception errors\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shapely\n",
    "import shapely.geometry\n",
    "\n",
    "def __get_valid_wkt_str(input, precision, debug_print):\n",
    "    \"\"\"\n",
    "    Function that checks that a wkt str is valid\n",
    "    \"\"\"\n",
    "    if type(input) is str:\n",
    "        wkt_str = input\n",
    "    else:\n",
    "        #Get the initial str\n",
    "        wkt_str = shapely.wkt.dumps(input, rounding_precision=precision)\n",
    "    #Loop until we find a valid polygon\n",
    "    for i in range(100):\n",
    "        polygon = shapely.wkt.loads(wkt_str)\n",
    "        if not polygon.is_valid:\n",
    "            #print debugging info\n",
    "            print 'Invalid polygon in %s. Iteration: %i' % (debug_print, i)\n",
    "            #Use buffer to try to fix the polygon\n",
    "            polygon = polygon.buffer(0)\n",
    "            #Get back to multipolygon if necesary\n",
    "            if polygon.type == 'Polygon':\n",
    "                polygon = shapely.geometry.MultiPolygon([polygon])\n",
    "            #Dump to str again\n",
    "            wkt_str = shapely.wkt.dumps(polygon, rounding_precision=precision)\n",
    "        else:\n",
    "            break\n",
    "    return wkt_str\n",
    "\n",
    "    \n",
    "def __create_square_around_point(point, side):\n",
    "    \"\"\"\n",
    "    Creates a square polygon with shapely given \n",
    "    the center point\n",
    "    \"\"\"\n",
    "    #Create canonical square points\n",
    "    square_points = np.zeros((4,2))\n",
    "    square_points[1, 0] = 1\n",
    "    square_points[2] = 1\n",
    "    square_points[3, 1] = 1\n",
    "    #Scale the square\n",
    "    square_points *= side\n",
    "    #Position to have the point in the center\n",
    "    for i in range(2):\n",
    "        square_points[:, i] += point[i] - side/2.\n",
    "    pol = shapely.geometry.Polygon(square_points)\n",
    "    return pol    \n",
    "    \n",
    "def repair_topology_exception(submission_path, precision, image_id, n_class, point, \n",
    "                              side=1e-4):\n",
    "    \"\"\"\n",
    "    Tries to repair the topology exception error by creating a squared \n",
    "    hole in the given point with the given side\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    #Load the submission\n",
    "    print 'Loading the submission...'\n",
    "    df = pd.read_csv(submission_path)\n",
    "    #Loop over the values changing them\n",
    "    #I'm going to use the values because I think iterating over the dataframe is slow\n",
    "    #But I'm not sure\n",
    "    print 'Looping over the polygons'\n",
    "    polygons = df.MultipolygonWKT.values\n",
    "    img_ids = df.ImageId.values\n",
    "    class_types = df.ClassType.values\n",
    "    for i, polygon, img_id, class_type in zip(range(len(polygons)), polygons,\n",
    "                                           img_ids, class_types):\n",
    "        if img_id == image_id and n_class == class_type:\n",
    "            polygon = shapely.wkt.loads(polygon)\n",
    "            square = __create_square_around_point(point, side)\n",
    "            polygon = polygon.difference(square)\n",
    "            polygons[i] = __get_valid_wkt_str(polygon, precision,\n",
    "                '%s class%i' % (img_id, class_type))\n",
    "    #Update the dataframe\n",
    "    df.MultipolygonWKT = polygons\n",
    "    #Save to a new file\n",
    "    print 'Saving the submission...'\n",
    "    df.to_csv(submission_path,\n",
    "              index=False)\n",
    "    print 'It took %i seconds to repair the submission' % (\n",
    "        time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-22T21:17:13.526598",
     "start_time": "2017-02-22T21:17:10.775401"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "repair_topology_exception('../output/my_unet37_125e.csv', \n",
    "                              precision=6, \n",
    "                              image_id='6100_0_2', \n",
    "                              n_class=2, \n",
    "                              point= (0.0080394952501907386 , -0.0034979715011444375), \n",
    "                              side=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:00:21.253972",
     "start_time": "2017-02-23T14:00:20.983304"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "output = pd.read_csv(\"../input/sample_submission.csv\", dtype={'ImageId':'str','ClassType':'int','MultipolygonWKT':'str'})\n",
    "test_list = list(output['ImageId'].unique())\n",
    "\n",
    "model.load_weights('../weights/unet37_125e')\n",
    "\n",
    "# good old threshold\n",
    "val_tr_list = np.array([ 0.386,  0.198,  0.45 ,  0.258,  0.408,  0.514,  0.46 ,  0.192,\n",
    "        0.008,  0.07 ])\n",
    "tr_list = np.array([ 0.378,  0.2  ,  0.608,  0.324,  0.414,  0.496,  0.482,  0.442,\n",
    "        0.098,  0.09 ])\n",
    "threshold = 0.2*tr_list + 0.8*val_tr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:00:22.771280",
     "start_time": "2017-02-23T14:00:22.168251"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SatelliteImg(object):\n",
    "\n",
    "    def __init__(self, image_id, high, low, train = True):\n",
    "        self.img = np.empty((image_size,image_size,0))\n",
    "        self.img2 = np.empty((image_size,image_size,0))\n",
    "        j = 0\n",
    "        for band in bands:\n",
    "            img = self.get_img(image_id, band)\n",
    "            channel = img.shape[2]\n",
    "            img_resized1 = np.zeros((image_size,image_size,channel))\n",
    "            img_resized2 = np.zeros((image_size,image_size,channel))\n",
    "            # resize image to standard size = 1024*1024\n",
    "            for i in range(channel):\n",
    "                img_resized1[:,:,i] = cv2.resize(img[:,:,i], (image_size,image_size))\n",
    "                img_resized2[:,:,i] = img_resized1[:,:,i]\n",
    "                img_resized1[:,:,i] = img_resized1[:,:,i].clip(min=low[j], max=high[j])     \n",
    "                img_resized1[:,:,i] = 255.0*(img_resized1[:,:,i] - low[j])/ (high[j]-low[j]) \n",
    "                j += 1          \n",
    "            self.img = np.append(self.img, img_resized1, axis=-1)\n",
    "            self.img2 = np.append(self.img2, img_resized2, axis = -1)\n",
    "        \n",
    "        self.channel = self.img.shape[2]\n",
    "        self.x_pixel, self.y_pixel = self.img.shape[:2]\n",
    "        self.img_size = self.x_pixel*self.y_pixel\n",
    "        for _im_id, _x, _y in csv.reader(open('../input/grid_sizes.csv')):\n",
    "            if _im_id == image_id:\n",
    "                self.x_max, self.y_min = float(_x), float(_y)\n",
    "                break   \n",
    "        self.x_scaler, self.y_scaler = self.get_scalers() \n",
    "        \n",
    "        self.train_polygons = [None]*10\n",
    "        self.train_polygons_scaled = [None]*10\n",
    "        self.train_mask = np.zeros((self.x_pixel, self.y_pixel, N_Cls))\n",
    "        \n",
    "        if train == True:\n",
    "            for poly_type in range(0,N_Cls):\n",
    "                for _im_id, _poly_type, _poly in csv.reader(open('../input/train_wkt_v4.csv')):\n",
    "                    if _im_id == image_id and _poly_type == str(poly_type+1):                    \n",
    "                        self.train_polygons[poly_type] = shapely.wkt.loads(_poly)\n",
    "                        self.train_polygons_scaled[poly_type] = shapely.affinity.scale(\n",
    "                            self.train_polygons[poly_type], xfact=self.x_scaler, yfact=self.y_scaler, origin=(0, 0, 0))      \n",
    "                        self.train_mask[:,:,poly_type] = self.mask_for_polygons(self.train_polygons_scaled[poly_type])\n",
    "                        break\n",
    "\n",
    "    def stretch_8bit(self, bands, lower_percent=2, higher_percent=98):\n",
    "        out = np.zeros_like(bands)\n",
    "        x,y,z = bands.shape\n",
    "        for i in range(z):\n",
    "            a = 0 \n",
    "            b = 255 \n",
    "            c = np.percentile(bands[:,:,i], lower_percent)\n",
    "            d = np.percentile(bands[:,:,i], higher_percent)        \n",
    "            t = a + (bands[:,:,i] - c) * (b - a) / (d - c)    \n",
    "            t[t<a] = a\n",
    "            t[t>b] = b\n",
    "            out[:,:,i] = t\n",
    "        return out.astype(np.uint8)    \n",
    "    \n",
    "    def get_img(self, image_id, band = 'M'):\n",
    "        if band == 'M':\n",
    "            filename = os.path.join('..', 'input', 'sixteen_band', '{}_M.tif'.format(image_id))\n",
    "        elif band == 'A':\n",
    "            filename = os.path.join('..', 'input', 'sixteen_band', '{}_A.tif'.format(image_id)) \n",
    "        elif band == 'P':\n",
    "            filename = os.path.join('..', 'input', 'sixteen_band', '{}_P.tif'.format(image_id))\n",
    "        elif band == 'rgb':\n",
    "            filename = os.path.join('..', 'input', 'three_band', '{}.tif'.format(image_id))\n",
    "        img = tiff.imread(filename) \n",
    "        if band == 'P':\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "        img = np.rollaxis(img, 0, 3)\n",
    "        return img\n",
    "    \n",
    "    def get_scalers(self):\n",
    "        w = float(self.y_pixel)\n",
    "        h = float(self.x_pixel)\n",
    "        w_ = w * (w / (w + 1))\n",
    "        h_ = h * (h / (h + 1))\n",
    "        return w_ / self.x_max, h_ / self.y_min\n",
    "    \n",
    "    def mask_for_polygons(self, polygons):\n",
    "        img_mask = np.zeros((self.x_pixel,self.y_pixel), np.uint8)\n",
    "        if not polygons:\n",
    "            return img_mask\n",
    "        int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "        exteriors = [int_coords(poly.exterior.coords) for poly in polygons]\n",
    "        interiors = [int_coords(pi.coords) for poly in polygons for pi in poly.interiors]\n",
    "        cv2.fillPoly(img_mask, exteriors, 1)\n",
    "        cv2.fillPoly(img_mask, interiors, 0)\n",
    "        return img_mask\n",
    "\n",
    "    def show_img(self):\n",
    "        m = self.img\n",
    "        img = np.zeros_like(m[:,:,0:3])\n",
    "        img[:,:,0] = m[:,:,4] #red\n",
    "        img[:,:,1] = m[:,:,2] #green\n",
    "        img[:,:,2] = m[:,:,1] #blue\n",
    "        img = self.stretch_8bit(img) # simple strech is probably not a good idea for learning        \n",
    "        return plt.imshow(img) \n",
    "    \n",
    "    def get_mask(self):\n",
    "        for i in range(N_Cls):\n",
    "            self.train_polygons_scaled[i] = shapely.affinity.scale(\n",
    "                            self.train_polygons[i], xfact=self.x_scaler, yfact=self.y_scaler, origin=(0, 0, 0))\n",
    "            self.train_mask[:,:,i] = self.mask_for_polygons(self.train_polygons_scaled[i])\n",
    "        return self.train_mask\n",
    "    \n",
    "    def show_mask(self, poly_type=1):\n",
    "        #train_mask = self.mask_for_polygons(self.train_polygons_scaled[poly_type])\n",
    "        train_mask = self.train_mask[:,:,poly_type]\n",
    "        return tiff.imshow(255 * np.stack([train_mask, train_mask, train_mask]))   \n",
    "    \n",
    "    def get_polygon(self, poly_type=1):\n",
    "        return self.train_polygons[poly_type]\n",
    "    \n",
    "    def get_data(self):\n",
    "        #img = self.stretch_8bit(self.img)\n",
    "        img = self.img\n",
    "        img = img.reshape((self.x_pixel*self.y_pixel, self.channel))\n",
    "        df = pd.DataFrame(img,index=range(self.img_size))\n",
    "        return df\n",
    "\n",
    "    def get_patch(self):\n",
    "        img1 = self.img\n",
    "        #img2 = self.img2\n",
    "        mask = self.train_mask   \n",
    "        #x1, x2, y, label = [],[], [], []\n",
    "        x1, y, label = [],[], []\n",
    "        tr = [0.04, 0.01, 0.009, 0.03, 0.12, 0.27, 0.005, 0.002, 0.00003, 0.00036] \n",
    "        no_patch = int(1.0*image_size/patch_size)\n",
    "        \n",
    "        for i in range(no_patch):\n",
    "            for j in range(no_patch):\n",
    "                xc = i*patch_size\n",
    "                yc = j*patch_size\n",
    "                im1 = img1[xc:xc + patch_size, yc:yc + patch_size,:] \n",
    "                #im2 = img2[xc:xc + patch_size, yc:yc + patch_size,:] \n",
    "                ms = mask[xc:xc + patch_size, yc:yc + patch_size,:]\n",
    "                obj = []\n",
    "                for k in range(N_Cls):\n",
    "                    sm = np.sum(ms[:, :, k])\n",
    "                    frac = float(sm) / (patch_size**2)\n",
    "                    obj.append(frac) \n",
    "                x1.append(im1)\n",
    "                #x2.append(im2)\n",
    "                y.append(ms)\n",
    "                test = [float(m - n)/n for m,n in zip(obj,tr)]\n",
    "                label.append(test.index(max(test)))\n",
    "                  \n",
    "        #return x1,x2, y, label\n",
    "        return x1, y, label\n",
    "\n",
    "cdict = {'red':[255,1,1],'purple':[200,40,194],'grey':[128,128,128],'brown':[102,51,1],'green':[75,212,65],\n",
    "         'ggreen':[204,155,153],'blue':[56,88,215],'navy':[1,1,102],'yellow':[153,153,1],'yyellow':[255,255,102]}\n",
    "\n",
    "def color(img, rgb=cdict['grey']):\n",
    "    result = np.zeros((1024,1024,4))\n",
    "    result[:,:,0:3] +=  np.transpose(np.stack([rgb[0]*img, rgb[1]*img, rgb[2]*img]),(1,2,0))/255.0\n",
    "    result[:,:,3] += 0.7*img\n",
    "       \n",
    "    return result.astype(np.float32)\n",
    "\n",
    "def stretch_8bit(bands, lower_percent=2, higher_percent=98):\n",
    "    out = np.zeros_like(bands)\n",
    "    x,y,z = bands.shape\n",
    "\n",
    "    for i in range(z):\n",
    "        a = 0 \n",
    "        b = 255 \n",
    "        c = np.percentile(bands[:,:,i], lower_percent)\n",
    "        d = np.percentile(bands[:,:,i], higher_percent)        \n",
    "        t = a + (bands[:,:,i] - c) * (b - a) / (d - c)    \n",
    "        t[t<a] = a\n",
    "        t[t>b] = b\n",
    "        out[:,:,i] = t\n",
    "        \n",
    "    if out.shape[2] != 1:\n",
    "        output = out.astype(np.uint8) \n",
    "    else:\n",
    "        output = out[:,:,0].astype(np.uint8) \n",
    "    return output\n",
    "\n",
    "def get_rgb(image_id):\n",
    "    img = tiff.imread(\"../input/three_band/{}.tif\".format(image_id))\n",
    "    img = np.transpose(img,(1,2,0))\n",
    "    img = stretch_8bit(img)\n",
    "    rgb = np.zeros((image_size,image_size,3))\n",
    "    \n",
    "    for i in range(3):\n",
    "        rgb[:,:,i] = cv2.resize(img[:,:,i], (image_size,image_size)) \n",
    "        \n",
    "    return rgb.astype(np.uint8)\n",
    "\n",
    "def get_color(instance):\n",
    "    dd = []\n",
    "    msk = instance.get_mask()\n",
    "    dd.append(color(msk[:,:,0],cdict['red']))\n",
    "    dd.append(color(msk[:,:,1],cdict['purple']))\n",
    "    dd.append(color(msk[:,:,2],cdict['grey']))\n",
    "    dd.append(color(msk[:,:,3],cdict['brown']))\n",
    "    dd.append(color(msk[:,:,4],cdict['green']))\n",
    "    dd.append(color(msk[:,:,5],cdict['ggreen']))\n",
    "    dd.append(color(msk[:,:,6],cdict['blue']))\n",
    "    dd.append(color(msk[:,:,7],cdict['navy']))\n",
    "    dd.append(color(msk[:,:,8],cdict['yellow']))\n",
    "    dd.append(color(msk[:,:,9],cdict['yyellow']))\n",
    "    \n",
    "    return dd\n",
    "\n",
    "def show_pics(image_id, dd, msk_no=0, y1=0,y2=image_size,x1=0,x2=image_size):\n",
    "    f, (real, mask) = plt.subplots(1, 2, figsize=(15,8))\n",
    "  \n",
    "    my_rgb = get_rgb(image_id)\n",
    "    real.imshow(my_rgb[y1:y2,x1:x2])\n",
    "    mask.imshow(my_rgb[y1:y2,x1:x2])\n",
    "    mask.imshow(dd[msk_no-1][y1:y2,x1:x2])\n",
    "    \n",
    "def show_pics_all(image_id, y1=0,y2=image_size,x1=0,x2=image_size):\n",
    "    f, (real, mask) = plt.subplots(1, 2, figsize=(30,16))\n",
    "\n",
    "    image_test = SatelliteImg(image_id, high = high, low = low, train = False)\n",
    "    data = image_test.img\n",
    "    pred = predict(model, data, threshold)\n",
    "\n",
    "    for i in range(N_Cls):        \n",
    "        image_test.train_polygons[i] = to_polygons(pred[:,:,i], image_test, epsilon=1., min_area=5.)\n",
    "    dd = get_color(image_test)\n",
    "\n",
    "    my_rgb = get_rgb(image_id)\n",
    "    real.imshow(my_rgb[y1:y2,x1:x2])\n",
    "    mask.imshow(my_rgb[y1:y2,x1:x2])\n",
    "    mask.imshow(dd[0][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[1][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[2][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[3][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[4][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[5][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[6][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[7][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[8][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[9][y1:y2,x1:x2])\n",
    "    \n",
    "    f.suptitle(image_id)\n",
    "    \n",
    "    f.savefig('../visualization_pred37/{}.jpg'.format(image_id))\n",
    "    \n",
    "def show_pics_label(image_id, y1=0,y2=image_size,x1=0,x2=image_size):\n",
    "    f, (real, mask) = plt.subplots(1, 2, figsize=(30,16))\n",
    "\n",
    "    image_test = SatelliteImg(image_id, high = high, low = low, train = True)\n",
    "    dd = get_color(image_test)\n",
    "\n",
    "    my_rgb = get_rgb(image_id)\n",
    "    real.imshow(my_rgb[y1:y2,x1:x2])\n",
    "    mask.imshow(my_rgb[y1:y2,x1:x2])\n",
    "    mask.imshow(dd[0][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[1][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[2][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[3][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[4][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[5][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[6][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[7][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[8][y1:y2,x1:x2])\n",
    "    mask.imshow(dd[9][y1:y2,x1:x2])\n",
    "    f.suptitle(image_id)\n",
    "    \n",
    "    f.savefig('../visualization_label/{}.jpg'.format(image_id))\n",
    "    \n",
    "def show_all_label(img_list):\n",
    "\n",
    "    for i,img in enumerate(img_list):\n",
    "        show_pics_label(img) \n",
    "\n",
    "def show_all_pred(img_list):\n",
    "\n",
    "    for i,img in enumerate(img_list):\n",
    "        show_pics_all(img)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T09:42:29.720433",
     "start_time": "2017-02-23T09:42:26.348578"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_pics_all('6100_0_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-02-23T03:53:08.106Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_all_label(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:00:33.039131",
     "start_time": "2017-02-23T14:00:33.033801"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def merge_images(file_list):\n",
    "    file_list.sort()\n",
    "    no_img = len(file_list)\n",
    "    test = Image.open('../visualization_label/6010_1_2.jpg')\n",
    "    (width, height) = test.size\n",
    "    result = Image.new('RGB', (width, no_img*height))\n",
    "    \n",
    "    \n",
    "    for i,myfile in enumerate(file_list):\n",
    "        print i\n",
    "        img = Image.open('../visualization_pred37/{}.jpg'.format(myfile))\n",
    "        result.paste(im=img, box=(0, i*height))\n",
    " \n",
    "    result.save('../visualization_pred37/all.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:15:38.119611",
     "start_time": "2017-02-23T14:15:27.240064"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_images(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:02:44.622499",
     "start_time": "2017-02-23T14:01:23.042511"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_all_pred(test_list[358:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T14:15:55.674565",
     "start_time": "2017-02-23T14:15:55.666060"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_list.index('6160_0_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T12:21:10.881865",
     "start_time": "2017-02-23T12:21:10.878966"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T12:24:45.216277",
     "start_time": "2017-02-23T12:24:45.211546"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd ../visualization_pred37/\n",
    "g = glob('*.*')\n",
    "%cd $cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-23T12:24:50.890620",
     "start_time": "2017-02-23T12:24:50.887140"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
